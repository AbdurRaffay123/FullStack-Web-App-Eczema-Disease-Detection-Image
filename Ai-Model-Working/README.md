# AI Model Working - Eczema Detection Microservice

FastAPI microservice for eczema detection using TensorFlow/Keras models with LLM reasoning.

## ğŸ—ï¸ Architecture

```
Ai-Model-Working/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                 # FastAPI application
â”‚   â”œâ”€â”€ schemas/
â”‚   â”‚   â””â”€â”€ response.py         # Pydantic response models
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ model_service.py    # TensorFlow model loading & inference
â”‚   â”‚   â”œâ”€â”€ relevance_detector.py # Image relevance detection
â”‚   â”‚   â”œâ”€â”€ severity_estimator.py # Severity estimation
â”‚   â”‚   â””â”€â”€ llm_service.py      # Bytez SDK + Gemma LLM
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ image_processor.py   # Image preprocessing
â”œâ”€â”€ models/                      # Model files directory
â”‚   â””â”€â”€ eczema_detector_efficientnet.h5
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â”œâ”€â”€ Dockerfile
â””â”€â”€ README.md
```

## ğŸš€ Quick Start

### 1. Install Dependencies

```bash
cd Ai-Model-Working
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 2. Configure Environment

```bash
cp .env.example .env
# Edit .env with your configuration
```

### 3. Place Model File

Place your `eczema_detector_efficientnet.h5` model file in:
```
Ai-Model-Working/models/eczema_detector_efficientnet.h5
```

### 4. Run Service

```bash
python -m app.main
# Or
uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
```

Service will be available at: `http://localhost:8000`

## ğŸ“¡ API Endpoints

### Health Check
```
GET /health
```

### Analyze Image
```
POST /analyze
Content-Type: multipart/form-data

Body: file (image file)
```

**Response:**
```json
{
  "relevant": true,
  "eczema_detected": true,
  "confidence": 0.87,
  "severity": "Moderate",
  "explanation": "The image shows skin patterns that moderately resemble eczema...",
  "disclaimer": "This is an AI-based assessment and not a medical diagnosis."
}
```

## ğŸ”Œ Integration with Node.js Backend

The Node.js backend can call this service:

```javascript
const FormData = require('form-data');
const fs = require('fs');
const axios = require('axios');

async function analyzeImage(imagePath) {
  const form = new FormData();
  form.append('file', fs.createReadStream(imagePath));
  
  const response = await axios.post(
    'http://localhost:8000/analyze',
    form,
    { headers: form.getHeaders() }
  );
  
  return response.data;
}
```

## ğŸ³ Docker Support

```bash
docker build -t eczema-ai-service .
docker run -p 8000:8000 eczema-ai-service
```

## âš™ï¸ Configuration

Environment variables (`.env`):
- `FASTAPI_HOST`: Host (default: 0.0.0.0)
- `FASTAPI_PORT`: Port (default: 8000)
- `MODEL_PATH`: Path to model file
- `MODEL_INPUT_SIZE`: Input size (default: 224)
- `BYTEZ_API_KEY`: Bytez API key for LLM
- `BYTEZ_MODEL`: Model name (default: google/gemma-3-27b-it)

## ğŸ›¡ï¸ Safety Features

- âœ… Image relevance detection (human skin only)
- âœ… Non-diagnostic language
- âœ… Confidence scores (not certainties)
- âœ… Safety disclaimers
- âœ… No medical advice

## ğŸ“ Notes

- Model loads once at startup for performance
- All image processing is async
- LLM explanations are optional (fallback available)
- Severity estimation uses heuristics, not medical diagnosis













